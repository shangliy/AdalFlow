{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f982335",
   "metadata": {},
   "source": [
    "\n",
    "# ðŸ§ª Self-Supervised Prompt Optimization (OvO) â€” Colab Notebook\n",
    "\n",
    "This Colab walks you through a **self-supervised prompt optimization** loop based on Output-vs-Output (OvO) pairwise judging.\n",
    "\n",
    "**What you'll get:**\n",
    "- A single-file runner with **early stop**, **retry/backoff**, **token-cost tracking**, and **judge/executor decoupling**\n",
    "- Example `tasks.json` and `prompt_seed.txt`\n",
    "- One-click run to produce `best_prompt.md`, `trace.json`, and `run_report.md`\n",
    "\n",
    "> Works with **Mistral** or any **OpenAI-compatible** endpoint. Just set your API key in the next cell.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea48c310",
   "metadata": {},
   "source": [
    "## 1) Setup API key and provider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034068ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "PROVIDER = \"mistral\"  # or \"openai\"\n",
    "os.environ[\"MISTRAL_API_KEY\"] = os.getenv(\"MISTRAL_API_KEY\", \"sk-REPLACE_ME\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
    "\n",
    "print(\"Provider:\", PROVIDER)\n",
    "print(\"MISTRAL_API_KEY set?\", bool(os.environ.get(\"MISTRAL_API_KEY\")))\n",
    "print(\"OPENAI_API_KEY set?\", bool(os.environ.get(\"OPENAI_API_KEY\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54aebd4f",
   "metadata": {},
   "source": [
    "## 2) Install minimal dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d28125",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip -q install requests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ab742c",
   "metadata": {},
   "source": [
    "## 3) Prepare example tasks and initial prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94823c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json, textwrap, pathlib\n",
    "\n",
    "example_tasks = [\n",
    "  {\"input\": \"Rewrite the product description in a friendly tone for parents of 6â€“10 year-old kids.\"},\n",
    "  {\"input\": \"Summarize the following paragraph into 3 bullet points: Paste your text here.\"},\n",
    "  {\"input\": \"Classify the intent (support/sales/other): 'I cannot access premium features after payment.'\"},\n",
    "  {\"input\": \"Extract a JSON with keys {company, title, years} from this bio: Paste bio text here.\"},\n",
    "  {\"input\": \"Turn these notes into a clear email with subject + body: Paste notes here.\"}\n",
    "]\n",
    "\n",
    "pathlib.Path(\"tasks.json\").write_text(json.dumps(example_tasks, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "prompt_seed = textwrap.dedent(\"\"\"\n",
    "You are a senior writing & information assistant.\n",
    "\n",
    "GOAL\n",
    "- Produce accurate, concise, and well-structured outputs that directly satisfy the Task.\n",
    "- Prefer bullet points and JSON where appropriate.\n",
    "- Never invent facts; if input is ambiguous, ask 1 brief clarification question.\n",
    "\n",
    "INPUT FORMAT\n",
    "- You will receive: ### Task ... ### Answer\n",
    "- Read Task carefully. If it requests JSON, return valid JSON only.\n",
    "\n",
    "STYLE & QUALITY\n",
    "- Clarity > verbosity. Use plain language.\n",
    "- If transforming text, preserve meaning and key details.\n",
    "- For summaries: 3â€“5 bullets, each â‰¤ 18 words.\n",
    "- For classifications: return a single lowercase label from the allowed set.\n",
    "\n",
    "SAFETY & HONESTY\n",
    "- If essential info is missing, return: \"NEED-CLARIFICATION: <one short question>\"\n",
    "\n",
    "Now wait for Task and then produce the Answer.\n",
    "\"\"\")\n",
    "\n",
    "pathlib.Path(\"prompt_seed.txt\").write_text(prompt_seed, encoding=\"utf-8\")\n",
    "\n",
    "print(\"Wrote tasks.json and prompt_seed.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e97910",
   "metadata": {},
   "source": [
    "## 4) Create the SPO runner (single-file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242d82cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%bash\n",
    "cat > spo_runner.py << 'PYCODE'\n",
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os, json, time, random, argparse, pathlib, sys\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import List, Dict, Any, Tuple\n",
    "import requests\n",
    "\n",
    "def _chat(url, api_key, model, messages, max_tokens=512, temperature=0.7, timeout=60):\n",
    "    headers = {\"Authorization\": f\"Bearer {api_key}\", \"Content-Type\": \"application/json\"}\n",
    "    payload = {\"model\": model, \"messages\": messages, \"temperature\": temperature, \"max_tokens\": max_tokens}\n",
    "    resp = requests.post(url, headers=headers, json=payload, timeout=timeout)\n",
    "    resp.raise_for_status()\n",
    "    data = resp.json()\n",
    "    text = data[\"choices\"][0][\"message\"][\"content\"]\n",
    "    usage = data.get(\"usage\", {})\n",
    "    return text, usage\n",
    "\n",
    "PROVIDERS = {\n",
    "    \"mistral\": {\"base_url\": \"https://api.mistral.ai/v1/chat/completions\", \"key_env\": \"MISTRAL_API_KEY\"},\n",
    "    \"openai\":  {\"base_url\": \"https://api.openai.com/v1/chat/completions\", \"key_env\": \"OPENAI_API_KEY\"},\n",
    "}\n",
    "\n",
    "SYS_EXEC = \"You are a helpful expert that strictly follows the prompt to solve the task.\"\n",
    "SYS_JUDGE = \"You are an impartial judge. Given a TASK and two CANDIDATE ANSWERS, decide which is BETTER for the task.\\nReturn ONLY 'A' or 'B' on the first line. Then give ONE short sentence explaining why.\"\n",
    "\n",
    "def judge_prompt(task, ansA, ansB):\n",
    "    return [\n",
    "        {\"role\":\"system\",\"content\":SYS_JUDGE},\n",
    "        {\"role\":\"user\",\"content\":(\n",
    "            \"TASK:\\n\"+task+\"\\n\\nCANDIDATE A:\\n\"+ansA+\"\\n\\nCANDIDATE B:\\n\"+ansB+\"\\n\\n\"\n",
    "            \"Answer with 'A' or 'B' ONLY on the first line, then a brief reason.\"\n",
    "        )}\n",
    "    ]\n",
    "\n",
    "OPT_SYS = \"You are a prompt engineer. Improve the given PROMPT for higher quality, truthful, concise outputs.\"\n",
    "def opt_user(best_prompt, brief_feedback):\n",
    "    return (\n",
    "        \"Here is the current best PROMPT (triple backticks). Improve it while keeping task intent unchanged.\\n\"\n",
    "        \"Focus on: clarity, structure, constraints, evaluation rubric, step-by-step reasoning hints.\\n\"\n",
    "        f\"Feedback signals:\\n- {brief_feedback}\\n\\n\"\n",
    "        \"```PROMPT\\n\" + best_prompt + \"\\n```\"\n",
    "        \"\\nReturn ONLY the improved prompt.\"\n",
    "    )\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    provider: str\n",
    "    api_key: str\n",
    "    base_url: str\n",
    "    exec_model: str\n",
    "    judge_model: str\n",
    "    rounds: int = 10\n",
    "    k: int = 4\n",
    "    n: int = 3\n",
    "    patience: int = 2\n",
    "    max_tokens_exec: int = 512\n",
    "    max_tokens_judge: int = 256\n",
    "    temperature_exec: float = 0.2\n",
    "    temperature_judge: float = 0.0\n",
    "    seed: int = 42\n",
    "    out_dir: str = \"runs/colab\"\n",
    "\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "def ensure_dir(p): pathlib.Path(p).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def backoff_retry(fn, retries=3, base=1.4, max_wait=10, *args, **kwargs):\n",
    "    for i in range(retries+1):\n",
    "        try:\n",
    "            return fn(*args, **kwargs)\n",
    "        except Exception:\n",
    "            if i==retries: raise\n",
    "            import time; time.sleep(min(max_wait, base**i))\n",
    "\n",
    "def call_model(cfg: Config, model: str, sys_msg: str, user_msg: str, max_tokens: int, temperature: float):\n",
    "    def _do():\n",
    "        return _chat(cfg.base_url, cfg.api_key, model,\n",
    "                     [{\"role\":\"system\",\"content\":sys_msg},{\"role\":\"user\",\"content\":user_msg}],\n",
    "                     max_tokens, temperature)\n",
    "    text, usage = backoff_retry(_do)\n",
    "    in_tok = usage.get(\"prompt_tokens\", 0); out_tok = usage.get(\"completion_tokens\", 0)\n",
    "    return text.strip(), (in_tok + out_tok)\n",
    "\n",
    "def exec_once(cfg: Config, prompt: str, task_text: str):\n",
    "    msg = f\"{prompt}\\n\\n### Task\\n{task_text}\\n\\n### Answer\"\n",
    "    out, tokens = call_model(cfg, cfg.exec_model, SYS_EXEC, msg, cfg.max_tokens_exec, cfg.temperature_exec)\n",
    "    return out, tokens\n",
    "\n",
    "def judge_pair(cfg: Config, task_text: str, A: str, B: str):\n",
    "    import random\n",
    "    if random.random() < 0.5: A_, B_, swap = A, B, False\n",
    "    else: A_, B_, swap = B, A, True\n",
    "    jtxt, tokens = call_model(cfg, cfg.judge_model, SYS_JUDGE, judge_prompt(task_text, A_, B_)[1][\"content\"],\n",
    "                              cfg.max_tokens_judge, cfg.temperature_judge)\n",
    "    first_line = jtxt.splitlines()[0].strip().upper()\n",
    "    label = \"A\" if (\"A\" in first_line) else (\"B\" if \"B\" in first_line else \"A\")\n",
    "    if swap: label = \"A\" if label==\"B\" else \"B\"\n",
    "    return label, \"\", tokens\n",
    "\n",
    "def improve_prompts(cfg: Config, best_prompt: str, feedback: str, k: int):\n",
    "    cands = []\n",
    "    for _ in range(k):\n",
    "        cand, _ = call_model(cfg, cfg.exec_model, OPT_SYS, opt_user(best_prompt, feedback),\n",
    "                             max_tokens=512, temperature=0.7)\n",
    "        cand = cand.strip()\n",
    "        if cand and cand not in cands: cands.append(cand)\n",
    "    return cands\n",
    "\n",
    "def run(PROVIDER=\"mistral\", TASK_FILE=\"tasks.json\", INIT_PROMPT=\"prompt_seed.txt\",\n",
    "        ROUNDS=10, K=4, N=3, PATIENCE=2, OUT_DIR=\"runs/colab\"):\n",
    "    base = PROVIDERS[PROVIDER]\n",
    "    key = os.getenv(base[\"key_env\"], \"\")\n",
    "    if not key: raise RuntimeError(f\"Missing API key. Set {base['key_env']}\")\n",
    "    cfg = Config(provider=PROVIDER, api_key=key, base_url=base[\"base_url\"],\n",
    "                 exec_model=\"mistral-large-latest\" if PROVIDER==\"mistral\" else \"gpt-4o\",\n",
    "                 judge_model=\"mistral-small-latest\" if PROVIDER==\"mistral\" else \"gpt-4o-mini\")\n",
    "    cfg.rounds, cfg.k, cfg.n, cfg.patience, cfg.out_dir = ROUNDS, K, N, PATIENCE, OUT_DIR\n",
    "\n",
    "    ensure_dir(cfg.out_dir); set_seed(cfg.seed)\n",
    "    tasks = json.loads(pathlib.Path(TASK_FILE).read_text(encoding=\"utf-8\"))\n",
    "    init_prompt = pathlib.Path(INIT_PROMPT).read_text(encoding=\"utf-8\")\n",
    "\n",
    "    random.shuffle(tasks)\n",
    "    train, valid = tasks[: max(3, cfg.n*2)], tasks[max(3, cfg.n*2): max(3, cfg.n*2)+max(2, cfg.n)]\n",
    "    best_prompt = init_prompt\n",
    "    history = []\n",
    "    best_win = 0.0; no_gain = 0\n",
    "\n",
    "    for r in range(1, cfg.rounds+1):\n",
    "        pool = random.sample(train, k=min(cfg.n, len(train)))\n",
    "        feedback = f\"Prior best win rate={best_win:.2f}. Improve faithfulness, conciseness, coverage for tasks like: \" + \", \".join(t[\"input\"][:40]+\"...\" for t in pool)\n",
    "        cands = [best_prompt] + improve_prompts(cfg, best_prompt, feedback, cfg.k)\n",
    "\n",
    "        exec_tokens = 0; judge_tokens = 0\n",
    "        results = {i: [] for i in range(len(cands))}\n",
    "        for t in pool:\n",
    "            outs = []\n",
    "            for i, p in enumerate(cands):\n",
    "                y, tk = exec_once(cfg, p, t[\"input\"]); exec_tokens += tk; outs.append((i, y))\n",
    "            base_idx = 0\n",
    "            for i in range(1, len(outs)):\n",
    "                (ia, ya), (ib, yb) = outs[base_idx], outs[i]\n",
    "                lab, _, tkj = judge_pair(cfg, t[\"input\"], ya, yb); judge_tokens += tkj\n",
    "                win_idx = ia if lab==\"A\" else ib\n",
    "                results[win_idx].append(1)\n",
    "\n",
    "        winrates = {i: (sum(v)/max(1, len(v))) for i, v in results.items()}\n",
    "        best_idx = max(winrates, key=lambda i: winrates[i])\n",
    "        best_prompt = cands[best_idx]; best_round_win = winrates[best_idx]\n",
    "\n",
    "        if best_round_win > best_win + 1e-6: best_win, no_gain = best_round_win, 0\n",
    "        else: no_gain += 1\n",
    "\n",
    "        history.append({\n",
    "            \"round\": r, \"best_winrate\": best_round_win,\n",
    "            \"token_cost_exec\": exec_tokens, \"token_cost_judge\": judge_tokens,\n",
    "            \"chosen_prompt\": best_prompt\n",
    "        })\n",
    "\n",
    "        print(f\"[Round {r}] best_winrate={best_round_win:.2f}  exec_tokens={exec_tokens} judge_tokens={judge_tokens}\")\n",
    "        if no_gain >= cfg.patience:\n",
    "            print(f\"Early stop at round {r} (no gain {no_gain} >= {cfg.patience})\"); break\n",
    "\n",
    "    # Save outputs\n",
    "    import json\n",
    "    pathlib.Path(cfg.out_dir, \"trace.json\").write_text(json.dumps({\n",
    "        \"config\": asdict(cfg), \"history\": history, \"best_prompt\": best_prompt\n",
    "    }, indent=2), encoding=\"utf-8\")\n",
    "    pathlib.Path(cfg.out_dir, \"best_prompt.md\").write_text(best_prompt, encoding=\"utf-8\")\n",
    "    pathlib.Path(cfg.out_dir, \"run_report.md\").write_text(\n",
    "        \"\\n\".join([\n",
    "            \"# Run Report\", f\"- Rounds executed: {len(history)}\",\n",
    "            f\"- Best train winrate: {best_win:.2f}\",\n",
    "            f\"- Token cost (sum): exec={sum(h['token_cost_exec'] for h in history)}, judge={sum(h['token_cost_judge'] for h in history)}\",\n",
    "            \"- Output: best_prompt.md, trace.json\"\n",
    "        ]), encoding=\"utf-8\"\n",
    "    )\n",
    "    print(\"Saved outputs to\", cfg.out_dir)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "    ap = argparse.ArgumentParser()\n",
    "    ap.add_argument(\"--provider\", default=\"mistral\")\n",
    "    ap.add_argument(\"--task_file\", default=\"tasks.json\")\n",
    "    ap.add_argument(\"--init_prompt\", default=\"prompt_seed.txt\")\n",
    "    ap.add_argument(\"--rounds\", type=int, default=10)\n",
    "    ap.add_argument(\"--k\", type=int, default=4)\n",
    "    ap.add_argument(\"--n\", type=int, default=3)\n",
    "    ap.add_argument(\"--patience\", type=int, default=2)\n",
    "    ap.add_argument(\"--out_dir\", default=\"runs/colab\")\n",
    "    args = ap.parse_args()\n",
    "    run(args.provider, args.task_file, args.init_prompt,\n",
    "        args.rounds, args.k, args.n, args.patience, args.out_dir)\n",
    "PYCODE\n",
    "python - << 'PYTEST'\n",
    "from pathlib import Path\n",
    "print(\"Exists:\", Path(\"spo_runner.py\").exists())\n",
    "PYTEST\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d7fc4e",
   "metadata": {},
   "source": [
    "## 5) Run optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301c964f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ROUNDS, K, N, PATIENCE = 6, 4, 3, 2\n",
    "OUT_DIR = \"runs/colab\"\n",
    "!python spo_runner.py --provider $PROVIDER --task_file tasks.json --init_prompt prompt_seed.txt   --rounds $ROUNDS --k $K --n $N --patience $PATIENCE --out_dir $OUT_DIR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6d9ee2",
   "metadata": {},
   "source": [
    "## 6) Inspect outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1df44b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "print(Path(\"runs/colab/run_report.md\").read_text())\n",
    "print(\"\\n--- best_prompt.md (first 600 chars) ---\\n\")\n",
    "print(Path(\"runs/colab/best_prompt.md\").read_text()[:600])\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}